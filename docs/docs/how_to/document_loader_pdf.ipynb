{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3dd7178-8337-44f0-a468-bc1af5c0e811",
   "metadata": {},
   "source": [
    "# How to load PDFs\n",
    "\n",
    "[Portable Document Format (PDF)](https://en.wikipedia.org/wiki/PDF), standardized as ISO 32000, is a file format developed by Adobe in 1992 to present documents, including text formatting and images, in a manner independent of application software, hardware, and operating systems.\n",
    "\n",
    "This guide covers how to [load](/docs/concepts/document_loaders/) `PDF` documents into the LangChain [Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html) format that we use downstream.\n",
    "\n",
    "Text in PDFs is typically represented via text boxes. They may also contain images. A PDF parser might do some combination of the following:\n",
    "\n",
    "- Agglomerate text boxes into lines, paragraphs, and other structures via heuristics or ML inference;\n",
    "- Run [OCR](https://en.wikipedia.org/wiki/Optical_character_recognition) on images to detect text therein;\n",
    "- Classify text as belonging to paragraphs, lists, tables, or other structures;\n",
    "- Structure text into table rows and columns, or key-value pairs.\n",
    "- Use multimodal LLM to extrat the body, page by page\n",
    "\n",
    "PDF files are organized in pages. This is not a good strategy. Indeed, this approach creates memory gaps in RAG projects. If a paragraph spans two pages, the beginning of the paragraph is at the end of one page, while the rest is at the start of the next. With a page-based approach, there will be two separate chunks, each containing part of a sentence. The corresponding vectors won’t be relevant. These chunks are unlikely to be selected when there’s a question specifically about the split paragraph. If one of the chunks is selected, there’s little chance the LLM can answer the question. This issue is worsened by the injection of headers, footers (if parsers haven’t properly removed them), images, or tables at the end of a page, as most current implementations tend to do. \n",
    "\n",
    "Images and tables are difficult challenges for PDF parsers.\n",
    "\n",
    "Some parsers can retrieve images. The question is what to do with them. It may be interesting to apply an OCR algorithm to extract the textual content of images, or to use a multimodal LLM to request the description of each image. With the result of an image conversion, where do I place it in the document flow? At the end? At the risk of breaking the content of a paragraph present on several pages? Implementations try to find a neutral location, between two paragraphs, if possible.\n",
    "\n",
    "When it comes to extracting tables, some can do it, with varying degrees of success, with or without integrating the tables into the text flow. A Markdown table cannot describe combined cells, unlike an HTML table.\n",
    "\n",
    "Finally, the metadata extracted from PDF files by the various parsers varies. We propose a minimum set that parsers should offer:\n",
    "\n",
    "- `source`\n",
    "- `page`\n",
    "- `total_page`\n",
    "- `creationdate`\n",
    "- `creator`\n",
    "- `producer`\n",
    "\n",
    "Most parsers offer similar parameters, such as mode, which allows you to request the retrieval of one document per page (`mode=\"page\"`), or the entire file stream in a single document (`mode=\"single\"`). Other modes can return the structure of the document, following the identification of each component.\n",
    "\n",
    "LangChain tries to unify the different parsers, to facilitate migration from one to the other. Why is it important? Each has its own characteristics and strategies, more or less effective depending on the family of PDF files. One strategy is to identify the family of the PDF file (by inspecting the metadata or the content of the first page) and then select the most efficient parser in that case. By unifying parsers, the following code doesn't need to deal with the specifics of different parsers, as the result is similar for each. \n",
    "\n",
    "LangChain integrates with a host of PDF parsers. Some are simple and relatively low-level; others will support OCR and image-processing, or perform advanced document layout analysis. The right choice will depend on your needs. Below we enumerate the possibilities.\n",
    "\n",
    "We will demonstrate these approaches on a [sample file](https://github.com/langchain-ai/langchain/blob/master/libs/community/tests/integration_tests/examples/layout-parser-paper.pdf):"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "file_path = (\n",
    "    \"../../docs/integrations/document_loaders/example_data/layout-parser-paper.pdf\"\n",
    ")"
   ],
   "id": "7ec1eddef73eca64"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    ":::info A note on multimodal models\n",
    "\n",
    "Many modern LLMs support inference over multimodal inputs (e.g., images). In some applications -- such as question-answering over PDFs with complex layouts, diagrams, or scans -- it may be advantageous to skip the PDF parsing, instead casting a PDF page to an image and passing it to a model directly. We demonstrate an example of this in the [Use of multimodal models](/docs/how_to/document_loader_pdf/#use-of-multimodal-models) section below.\n",
    "\n",
    ":::\n",
    "\n",
    "## Simple and fast text extraction\n",
    "\n",
    "If you are looking for a simple string representation of text that is embedded in a PDF, the method below is appropriate. It will return a list of [Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html) objects-- one per page-- containing a single string of the page's text in the Document's `page_content` attribute. It will not parse text in images, tables or scanned PDF pages. Under the hood it uses the [pypdf](https://pypdf.readthedocs.io/en/stable/) Python library.\n",
    "\n",
    "LangChain [document loaders](/docs/concepts/document_loaders) implement `lazy_load` and its async variant, `alazy_load`, which return iterators of `Document` objects. We will use these below."
   ],
   "id": "21c9ba0aa49de9a6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "%pip install -qU langchain_community pypdf",
   "id": "b742c1ed9fda8ba7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from pprint import pprint\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(file_path)\n",
    "pages = []\n",
    "async for page in loader.alazy_load():\n",
    "    pages.append(page)"
   ],
   "id": "edd87605b7d1a46a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "pprint(pages[0].metadata)\n",
    "print(pages[0].page_content)"
   ],
   "id": "b81a1025132de879"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Note that the metadata of each document stores the corresponding page number.\n",
    "\n",
    "### Vector search over PDFs\n",
    "\n",
    "Once we have loaded PDFs into LangChain `Document` objects, we can index them (e.g., a RAG application) in the usual way. Below we use OpenAI embeddings, although any LangChain [embeddings](https://python.langchain.com/docs/concepts/embedding_models) model will suffice."
   ],
   "id": "89e816b3b2329eec"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "%pip install -qU langchain-openai",
   "id": "1416fe4ee37ec458"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")"
   ],
   "id": "8f6a9e773d93313b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "vector_store = InMemoryVectorStore.from_documents(pages, OpenAIEmbeddings())\n",
    "docs = vector_store.similarity_search(\"What is LayoutParser?\", k=2)\n",
    "for doc in docs:\n",
    "    print(f'Page {doc.metadata[\"page\"]}: {doc.page_content[:300]}\\n')"
   ],
   "id": "c92174e6a92fd0de"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Extract and analyse images\n",
    "\n"
   ],
   "id": "6d49ba69a73bc7f2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "%pip install -qU rapidocr-onnxruntime",
   "id": "bba167297068b834"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain_community.document_loaders.parsers.pdf import (\n",
    "    convert_images_to_text_with_rapidocr,\n",
    ")\n",
    "\n",
    "loader = PyPDFLoader(\n",
    "    file_path,\n",
    "    mode=\"page\",\n",
    "    extract_images=True,\n",
    "    images_to_text=convert_images_to_text_with_rapidocr(format=\"markdown\"),\n",
    ")\n",
    "docs = loader.load()\n",
    "print(docs[5].page_content)"
   ],
   "id": "a6cfde55c0a65bf1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "It is possible to ask a multimodal LLM to describe the image.",
   "id": "435c9502f24cce3d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"OpenAI API key =\")"
   ],
   "id": "fcc75ed84bb08e98"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain_community.document_loaders.parsers.pdf import (\n",
    "    convert_images_to_description,\n",
    ")\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "loader = PyPDFLoader(\n",
    "    file_path,\n",
    "    mode=\"page\",\n",
    "    extract_images=True,\n",
    "    images_to_text=convert_images_to_description(\n",
    "        model=ChatOpenAI(model=\"gpt-4o-mini\", max_tokens=1024), format=\"text\"\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "print(docs[5].page_content)"
   ],
   "id": "c016f4eb9a29c2b8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Extract tables\n",
    "\n",
    "Some parsers can extract tables. This is the case of `PDFPlumberLoader`\n"
   ],
   "id": "699073731662a94f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "%pip install -qU langchain_community pdfplumber",
   "id": "7a996fde187b536d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain_community.document_loaders import PDFPlumberLoader\n",
    "\n",
    "loader = PDFPlumberLoader(\n",
    "    file_path,\n",
    "    mode=\"page\",\n",
    "    extract_tables=\"markdown\",\n",
    ")\n",
    "docs = loader.load()\n",
    "print(docs[4].page_content)"
   ],
   "id": "1cc1a6b6c8fc6e98"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Layout analysis and extraction of text from images\n",
    "\n",
    "If you require a more granular segmentation of text (e.g., into distinct paragraphs, titles, tables, or other structures) or require extraction of text from images, the method below is appropriate. It will return a list of [Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html) objects, where each object represents a structure on the page. The Document's metadata stores the page number and other information related to the object (e.g., it might store table rows and columns in the case of a table object).\n",
    "\n",
    "Under the hood it uses the `langchain-unstructured` library. See the [integration docs](/docs/integrations/document_loaders/unstructured_file/) for more information about using [Unstructured](https://docs.unstructured.io/welcome) with LangChain.\n",
    "\n",
    "Unstructured supports multiple parameters for PDF parsing:\n",
    "- `strategy` (e.g., `\"auto\"`, `\"fast\"`, `\"ocr_only\"` or `\"hi-res\"`)\n",
    "- API or local processing. You will need an API key to use the API.\n",
    "\n",
    "The [hi-res](https://docs.unstructured.io/api-reference/how-to/choose-hi-res-model) strategy provides support for document layout analysis and OCR. We demonstrate it below via the API. See [local parsing](/docs/how_to/document_loader_pdf/#local-parsing) section below for considerations when running locally."
   ],
   "id": "88bca3a8ab5af262"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "%pip install -qU langchain-unstructured",
   "id": "affb07602a288330"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if \"UNSTRUCTURED_API_KEY\" not in os.environ:\n",
    "    os.environ[\"UNSTRUCTURED_API_KEY\"] = getpass.getpass(\"Unstructured API Key:\")"
   ],
   "id": "226f9c2ee2b3132f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "As before, we initialize a loader and load documents lazily:",
   "id": "770bc93d79d2867a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain_unstructured import UnstructuredLoader\n",
    "\n",
    "loader = UnstructuredLoader(\n",
    "    file_path=file_path,\n",
    "    strategy=\"hi_res\",\n",
    "    partition_via_api=True,\n",
    "    coordinates=True,\n",
    ")\n",
    "docs = []\n",
    "for doc in loader.lazy_load():\n",
    "    docs.append(doc)"
   ],
   "id": "dd6700c019f5df1b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Here we recover more than 100 distinct structures over the 16 page document:",
   "id": "732f04cc101a6966"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print(len(docs))",
   "id": "191f924443ba0828"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We can use the document metadata to recover content from a single page:",
   "id": "77a5d8a84fe96ab"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "first_page_docs = [doc for doc in docs if doc.metadata.get(\"page_number\") == 0]\n",
    "\n",
    "for doc in first_page_docs:\n",
    "    print(doc.page_content)"
   ],
   "id": "f41b10c7ac676042"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Extracting tables and other structures\n",
    "\n",
    "Each `Document` we load represents a structure, like a title, paragraph, or table.\n",
    "\n",
    "Some structures may be of special interest for indexing or question-answering tasks. These structures may be:\n",
    "1. Classified for easy identification;\n",
    "2. Parsed into a more structured representation.\n",
    "\n",
    "Below, we identify and extract a table:"
   ],
   "id": "1138684a520264b9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<details>\n",
    "<summary>Click to expand code for rendering pages</summary>"
   ],
   "id": "8586c73ccff251da"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "%pip install -qU matplotlib PyMuPDF pillow",
   "id": "2d1e76e9fbbd7e37"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import fitz\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def plot_pdf_with_boxes(pdf_page, segments):\n",
    "    pix = pdf_page.get_pixmap()\n",
    "    pil_image = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
    "\n",
    "    fig, ax = plt.subplots(1, figsize=(10, 10))\n",
    "    ax.imshow(pil_image)\n",
    "    categories = set()\n",
    "    category_to_color = {\n",
    "        \"Title\": \"orchid\",\n",
    "        \"Image\": \"forestgreen\",\n",
    "        \"Table\": \"tomato\",\n",
    "    }\n",
    "    for segment in segments:\n",
    "        points = segment[\"coordinates\"][\"points\"]\n",
    "        layout_width = segment[\"coordinates\"][\"layout_width\"]\n",
    "        layout_height = segment[\"coordinates\"][\"layout_height\"]\n",
    "        scaled_points = [\n",
    "            (x * pix.width / layout_width, y * pix.height / layout_height)\n",
    "            for x, y in points\n",
    "        ]\n",
    "        box_color = category_to_color.get(segment[\"category\"], \"deepskyblue\")\n",
    "        categories.add(segment[\"category\"])\n",
    "        rect = patches.Polygon(\n",
    "            scaled_points, linewidth=1, edgecolor=box_color, facecolor=\"none\"\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "    # Make legend\n",
    "    legend_handles = [patches.Patch(color=\"deepskyblue\", label=\"Text\")]\n",
    "    for category in [\"Title\", \"Image\", \"Table\"]:\n",
    "        if category in categories:\n",
    "            legend_handles.append(\n",
    "                patches.Patch(color=category_to_color[category], label=category)\n",
    "            )\n",
    "    ax.axis(\"off\")\n",
    "    ax.legend(handles=legend_handles, loc=\"upper right\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def render_page(doc_list: list, page_number: int, print_text=True) -> None:\n",
    "    pdf_page = fitz.open(file_path).load_page(page_number - 1)\n",
    "    page_docs = [\n",
    "        doc for doc in doc_list if doc.metadata.get(\"page_number\") == page_number\n",
    "    ]\n",
    "    segments = [doc.metadata for doc in page_docs]\n",
    "    plot_pdf_with_boxes(pdf_page, segments)\n",
    "    if print_text:\n",
    "        for doc in page_docs:\n",
    "            print(f\"{doc.page_content}\\n\")"
   ],
   "id": "2587cf7318135eab"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "</details>",
   "id": "1bb946cc345de78a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "render_page(docs, 5)",
   "id": "2789c8be1888b548"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Note that although the table text is collapsed into a single string in the document's content, the metadata contains a representation of its rows and columns:",
   "id": "7432b359a1c4a998"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from IPython.display import HTML, display\n",
    "\n",
    "segments = [\n",
    "    doc.metadata\n",
    "    for doc in docs\n",
    "    if doc.metadata.get(\"page_number\") == 5 and doc.metadata.get(\"category\") == \"Table\"\n",
    "]\n",
    "\n",
    "display(HTML(segments[0][\"text_as_html\"]))"
   ],
   "id": "2119580a626cba86"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<table><thead><tr><th colspan=\"3\">able 1. LUllclll 1ayoul actCCLloll 1110AdCs 111 L1C LayoOulralsel 1110U4cl 200</th></tr><tr><th>Dataset</th><th>| Base Model\\'|</th><th>Notes</th></tr></thead><tbody><tr><td>PubLayNet [38]</td><td>F/M</td><td>Layouts of modern scientific documents</td></tr><tr><td>PRImA</td><td>M</td><td>Layouts of scanned modern magazines and scientific reports</td></tr><tr><td>Newspaper</td><td>F</td><td>Layouts of scanned US newspapers from the 20th century</td></tr><tr><td>TableBank [18]</td><td>F</td><td>Table region on modern scientific and business document</td></tr><tr><td>HJDataset</td><td>F/M</td><td>Layouts of history Japanese documents</td></tr></tbody></table>",
   "id": "a562193d4c974378"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Extracting text from specific sections\n",
    "\n",
    "Structures may have parent-child relationships -- for example, a paragraph might belong to a section with a title. If a section is of particular interest (e.g., for indexing) we can isolate the corresponding `Document` objects.\n",
    "\n",
    "Below, we extract all text associated with the document's \"Conclusion\" section:"
   ],
   "id": "9ecc9e161c5dea9b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "render_page(docs, 14, print_text=False)",
   "id": "94908d5097b5db89"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "conclusion_docs = []\n",
    "parent_id = -1\n",
    "for doc in docs:\n",
    "    if doc.metadata[\"category\"] == \"Title\" and \"Conclusion\" in doc.page_content:\n",
    "        parent_id = doc.metadata[\"element_id\"]\n",
    "    if doc.metadata.get(\"parent_id\") == parent_id:\n",
    "        conclusion_docs.append(doc)\n",
    "\n",
    "for doc in conclusion_docs:\n",
    "    print(doc.page_content)"
   ],
   "id": "fb3b5c8e6676c922"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Extracting text from images\n",
    "\n",
    "OCR is run on images, enabling the extraction of text therein:"
   ],
   "id": "46909c119aa8d4df"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "render_page(docs, 11)",
   "id": "9038ca7186debbf5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Note that the text from the figure on the right is extracted and incorporated into the content of the `Document`.",
   "id": "183205ea7757c8f7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Local parsing\n",
    "\n",
    "Parsing locally requires the installation of additional dependencies.\n",
    "\n",
    "**Poppler** (PDF analysis)\n",
    "- Linux: `apt-get install poppler-utils`\n",
    "- Mac: `brew install poppler`\n",
    "- Windows: https://github.com/oschwartz10612/poppler-windows\n",
    "\n",
    "**Tesseract** (OCR)\n",
    "- Linux: `apt-get install tesseract-ocr`\n",
    "- Mac: `brew install tesseract`\n",
    "- Windows: https://github.com/UB-Mannheim/tesseract/wiki#tesseract-installer-for-windows\n",
    "\n",
    "We will also need to install the `unstructured` PDF extras:"
   ],
   "id": "b13695da1644829b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "%pip install -qU \"unstructured[pdf]\"",
   "id": "2713c8364b586bb0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We can then use the [UnstructuredLoader](https://python.langchain.com/api_reference/unstructured/document_loaders/langchain_unstructured.document_loaders.UnstructuredLoader.html) much the same way, forgoing the API key and `partition_via_api` setting:",
   "id": "fc75c955e3058e53"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "loader_local = UnstructuredLoader(\n",
    "    file_path=file_path,\n",
    "    strategy=\"hi_res\",\n",
    ")\n",
    "docs_local = []\n",
    "for doc in loader_local.lazy_load():\n",
    "    docs_local.append(doc)"
   ],
   "id": "6ad3457045865836"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The list of documents can then be processed similarly to those obtained from the API.\n",
    "\n",
    "## Use of multimodal models\n",
    "\n",
    "Many modern LLMs support inference over multimodal inputs (e.g., images). In some applications-- such as question-answering over PDFs with complex layouts, diagrams, or scans-- it may be advantageous to skip the PDF parsing, instead casting a PDF page to an image and passing it to a model directly. This allows a model to reason over the two dimensional content on the page, instead of a \"one-dimensional\" string representation.\n",
    "\n",
    "In principle we can use any LangChain [chat model](/docs/concepts/chat_models) that supports multimodal inputs. A list of these models is documented [here](/docs/integrations/chat/). Below we use OpenAI's `gpt-4o-mini`.\n",
    "\n",
    "First we define a short utility function to convert a PDF page to a base64-encoded image:"
   ],
   "id": "f0d8269765fde9fc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "%pip install -qU PyMuPDF pillow langchain-openai",
   "id": "2746aac2f238a226"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import base64\n",
    "import io\n",
    "\n",
    "import fitz\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def pdf_page_to_base64(pdf_path: str, page_number: int):\n",
    "    pdf_document = fitz.open(pdf_path)\n",
    "    page = pdf_document.load_page(page_number - 1)  # input is one-indexed\n",
    "    pix = page.get_pixmap()\n",
    "    img = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
    "\n",
    "    buffer = io.BytesIO()\n",
    "    img.save(buffer, format=\"PNG\")\n",
    "\n",
    "    return base64.b64encode(buffer.getvalue()).decode(\"utf-8\")"
   ],
   "id": "1b405b235fec6d46"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from IPython.display import Image as IPImage\n",
    "from IPython.display import display\n",
    "\n",
    "base64_image = pdf_page_to_base64(file_path, 11)\n",
    "display(IPImage(data=base64.b64decode(base64_image)))"
   ],
   "id": "21780bc75c00a634"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We can then query the model in the [usual way](/docs/how_to/multimodal_inputs/). Below we ask it a question on related to the diagram on the page.",
   "id": "33aaf63627020362"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")"
   ],
   "id": "b6adde8762f06e8a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "query = \"What is the name of the first step in the pipeline?\"\n",
    "\n",
    "message = HumanMessage(\n",
    "    content=[\n",
    "        {\"type\": \"text\", \"text\": query},\n",
    "        {\n",
    "            \"type\": \"image_url\",\n",
    "            \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"},\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "response = llm.invoke([message])\n",
    "print(response.content)"
   ],
   "id": "8c56231d45e98fbb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Other PDF loaders\n",
    "\n",
    "For a list of available LangChain PDF loaders, please see [this table](/docs/integrations/document_loaders/#pdfs)."
   ],
   "id": "8ed19235e5e266e1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d458df6d9142d2fe"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "patch-langchain",
   "language": "python",
   "name": "patch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
